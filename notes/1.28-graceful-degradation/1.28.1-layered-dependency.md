# Layered dependency and graceful degradation for Catena on BEAM

A functional programming language targeting BEAM can leverage deep category-theoretic foundations alongside battle-tested OTP patterns to implement a principled, type-safe system for managing service availability levels. This report presents a comprehensive design combining **graded monads** for compile-time availability tracking, **lattice structures** for modeling degradation states, and **OTP supervision trees** for runtime fault tolerance.

## The core problem demands categorical precision

Catena, as a category-theory-based language on BEAM, requires a graceful degradation system that goes beyond ad-hoc runtime checks. The system must track availability levels through types, model valid degradation transitions formally, and integrate seamlessly with OTP's "let it crash" philosophy. Three requirements emerge as fundamental: services must be partitioned into required versus optional at each level, the system must continue operating in degraded states when non-critical services fail, and availability level changes must propagate dynamically throughout the runtime.

The key insight is that **availability levels form a bounded lattice**, degradation transitions are **morphisms in a category**, and service dependencies with their effects can be tracked via **graded monads**. This formalization ensures that invalid states are unrepresentable and that degradation behavior is compositional.

## Availability levels as a bounded lattice structure

A bounded lattice `(L, ⊔, ⊓, ⊤, ⊥)` provides the mathematical foundation for availability states. For a three-level system:

```
L₂ (Full)      ⊤ = maximum availability
 │
L₁ (Degraded)  intermediate availability  
 │
L₀ (Minimal)   ⊥ = minimum availability
```

The partial ordering `L₀ ⊑ L₁ ⊑ L₂` captures "can operate with at least these capabilities." The **join** operation `⊔` represents combining availability requirements (a computation needing both L₁ and L₂ services requires L₂), while **meet** `⊓` represents the intersection of capabilities. This lattice structure enables the **Knaster-Tarski fixed-point theorem** to compute stable system states after cascading failures.

**Category Avail** formalizes this with objects `{L₀, L₁, L₂}` and morphisms representing valid degradation paths. Crucially, only **downward** morphisms exist (degradation is one-way without explicit recovery actions):

```
d₂₁ : L₂ → L₁    (degrade from full to degraded)
d₁₀ : L₁ → L₀    (degrade from degraded to minimal)
d₂₀ : L₂ → L₀    (direct degradation, d₂₀ = d₁₀ ∘ d₂₁)
```

## Graded monads track availability requirements in types

**Graded monads**, as formalized by Katsumata (2014) and implemented in the Haskell `effect-monad` package by Orchard and Petricek, provide the categorical structure for tracking which availability level a computation requires. A graded monad over a monoid `(E, •, ε)` consists of a family of functors `Gₑ` indexed by elements of E with:

- **Unit**: `η : A → G_ε A` (pure computation requiring no services)
- **Multiplication**: `μ : Gₑ(Gf A) → G_{e•f} A` (composing effects combines requirements)

For availability tracking, the monoid operation is **join** (maximum), ensuring that composing computations accumulates their requirements:

```haskell
-- Catena type-level availability tracking (conceptual syntax)
type family Max (a :: Level) (b :: Level) :: Level where
  Max L2 _  = L2
  Max _  L2 = L2  
  Max L1 _  = L1
  Max _  L1 = L1
  Max L0 L0 = L0

-- Graded monad for availability-aware computations
data Avail (level :: Level) a where
  Pure   :: a -> Avail L0 a
  Require :: ServiceCall level a -> Avail level a

-- Composition propagates maximum level
instance GradedMonad Avail where
  greturn = Pure
  gbind :: Avail e a -> (a -> Avail f b) -> Avail (Max e f) b
```

This approach means **type signatures reveal availability requirements**. A function with type `Avail L1 Response` cannot be called when the system is at L₀, and this constraint is enforced statically. The Orchard-Wadler-Eades paper "Unifying graded and parameterised monads" (2020) shows these structures extend to **category-graded monads** when state transitions need tracking alongside effects.

## Parameterised monads model state transitions explicitly

While graded monads track cumulative requirements, **parameterised (indexed) monads** from Atkey (2009) explicitly model state transitions:

```haskell
-- Indexed monad for availability state transitions
newtype ServiceM (pre :: Level) (post :: Level) a = 
  ServiceM { runServiceM :: Runtime -> IO (a, Runtime) }

class IxMonad m where
  ireturn :: a -> m i i a                         -- stay in same state
  ibind   :: m i j a -> (a -> m j k b) -> m i k b -- chain transitions

-- Only valid degradation transitions compile
degrade :: ServiceM L2 L1 ()  -- explicit: starts at L2, ends at L1
upgrade :: HealthCheck -> ServiceM L1 L2 (Maybe ())  -- recovery requires proof
```

This formulation makes **illegal state transitions unrepresentable**—you cannot write code that assumes L₂ availability after a degradation event. The McBride "Kleisli Arrows of Outrageous Fortune" technique provides an even more expressive indexed structure for tracking complex invariants.

## Service dependencies as a dependency functor

Service dependencies form a **directed acyclic graph** that maps functorially to availability levels. Define category **Svc** where objects are services and morphisms `f : A → B` indicate "A depends on B":

```
DatabasePool ← CacheLayer ← RecommendationEngine
      ↑              ↑
   CoreAPI      SearchService
```

A **dependency functor** `Dep : Svc → Avail` assigns each service its required availability level. The functor laws enforce a critical invariant: **a service cannot be at a higher availability level than its dependencies**. If `f : A → B` exists (A depends on B), then `Dep(A) ⊑ Dep(B)`.

```elixir
# Elixir representation of service dependency specification
defmodule ServiceSpec do
  defstruct [
    :name,
    :level,           # :l0 | :l1 | :l2
    :type,            # :critical | :optional
    :dependencies,    # [service_name]
    :capabilities,    # [:search, :recommendations, ...]
    :health_check,    # fn -> :ok | {:error, reason}
    :fallback         # fn -> fallback_response (for optional services)
  ]
end

@service_graph %{
  database:   %ServiceSpec{level: :l0, type: :critical, dependencies: []},
  cache:      %ServiceSpec{level: :l1, type: :critical, dependencies: [:database]},
  search:     %ServiceSpec{level: :l2, type: :optional, dependencies: [:cache]},
  ml_service: %ServiceSpec{level: :l2, type: :optional, dependencies: [:database]}
}
```

## Galois connections bridge concrete and abstract availability

A **Galois connection** between concrete system states and abstract availability levels enables principled abstraction. Let the concrete domain be `C = ServiceName → ServiceState` (full system configuration) and the abstract domain be `A = Level`:

```
α : C → A    (abstraction: compute minimum satisfied level)
γ : A → C    (concretization: all configs at least this level)

α(s) ⊑ l  ⟺  s ⊑ γ(l)    (adjunction property)
```

This structure, foundational to abstract interpretation (Cousot & Cousot), enables safe approximation of system behavior. If concrete state `s` satisfies `α(s) = L₁`, then all L₁ and L₀ functionality is guaranteed available:

```elixir
defmodule AvailabilityAbstraction do
  @level_requirements %{
    l2: [:database, :cache, :search, :ml_service],
    l1: [:database, :cache],
    l0: [:database]
  }

  # Abstraction function: concrete state → availability level
  def alpha(service_states) do
    up_services = for {name, :up} <- service_states, do: name
    
    Enum.find([:l2, :l1, :l0], :l0, fn level ->
      required = Map.get(@level_requirements, level, [])
      Enum.all?(required, &(&1 in up_services))
    end)
  end
  
  # Concretization: availability level → set of valid states
  def gamma(level) do
    required = Map.get(@level_requirements, level, [])
    # Returns predicate: fn state -> all required services up?
    fn state -> Enum.all?(required, &(Map.get(state, &1) == :up)) end
  end
end
```

## The Catena runtime architecture integrates three layers

The proposed architecture consists of three integrated layers working together:

**Layer 1: Compile-time availability tracking** uses graded types to ensure code cannot call services unavailable at its declared level. The Catena compiler rejects programs that violate availability constraints.

**Layer 2: Runtime availability manager** implements an OTP GenServer tracking current system level, service health, and broadcasting state changes. This layer uses GenStateMachine for well-defined state transitions.

**Layer 3: OTP supervision tree** partitions services by criticality level, using supervision strategies that match degradation semantics—`rest_for_one` for dependent chains, `one_for_one` for independent optional services.

```
Catena.Application
├── CriticalSupervisor (:rest_for_one, max_restarts: 3/60s)
│   ├── Catena.AvailabilityManager (GenStateMachine)
│   ├── Catena.Database.Pool
│   └── Catena.Core.Registry
├── Level1Supervisor (:one_for_one)
│   ├── Catena.Cache.Supervisor
│   └── Catena.HealthChecker
└── Level2Supervisor (:one_for_one, temporary children)
    ├── Catena.Search.Service
    ├── Catena.ML.Service
    └── Catena.Recommendations.Service
```

## The availability manager as a GenStateMachine

The core runtime component implements availability state transitions using Elixir's GenStateMachine, which provides explicit state machine semantics:

```elixir
defmodule Catena.AvailabilityManager do
  use GenStateMachine, callback_mode: [:state_functions, :state_enter]
  
  defstruct [
    :services,           # %{name => :up | :down | :degraded}
    :level,              # current availability level
    :subscribers,        # PIDs to notify on changes
    :level_requirements  # %{level => [required_services]}
  ]

  # State: l2_available (full availability)
  def l2_available(:enter, _old_state, data) do
    broadcast_level_change(data, :l2)
    {:keep_state, data}
  end
  
  def l2_available(:cast, {:service_down, service}, data) do
    new_services = Map.put(data.services, service, :down)
    new_level = calculate_level(new_services, data.level_requirements)
    
    case new_level do
      :l2 -> {:keep_state, %{data | services: new_services}}
      :l1 -> {:next_state, :l1_degraded, %{data | services: new_services, level: :l1}}
      :l0 -> {:next_state, :l0_minimal, %{data | services: new_services, level: :l0}}
    end
  end
  
  def l2_available({:call, from}, :get_level, data) do
    {:keep_state_and_data, [{:reply, from, :l2}]}
  end

  # State: l1_degraded (degraded availability)
  def l1_degraded(:enter, _old_state, data) do
    broadcast_level_change(data, :l1)
    disable_l2_features()
    {:keep_state, data}
  end
  
  def l1_degraded(:cast, {:service_up, service}, data) do
    new_services = Map.put(data.services, service, :up)
    new_level = calculate_level(new_services, data.level_requirements)
    
    if new_level == :l2 do
      {:next_state, :l2_available, %{data | services: new_services, level: :l2}}
    else
      {:keep_state, %{data | services: new_services}}
    end
  end
  
  def l1_degraded(:cast, {:service_down, service}, data) when is_critical?(service, :l1) do
    new_services = Map.put(data.services, service, :down)
    {:next_state, :l0_minimal, %{data | services: new_services, level: :l0}}
  end

  # State: l0_minimal (minimal availability)
  def l0_minimal(:enter, _old_state, data) do
    broadcast_level_change(data, :l0)
    enter_maintenance_mode()
    {:keep_state, data}
  end
  
  defp calculate_level(services, requirements) do
    up_services = for {name, status} <- services, status == :up, do: name
    
    cond do
      satisfies_level?(up_services, requirements, :l2) -> :l2
      satisfies_level?(up_services, requirements, :l1) -> :l1
      true -> :l0
    end
  end
  
  defp broadcast_level_change(data, new_level) do
    Phoenix.PubSub.broadcast(Catena.PubSub, "availability", {:level_changed, new_level})
  end
end
```

## Health checking with circuit breakers protects against cascading failures

Each external dependency wraps in a circuit breaker (using the battle-tested Erlang `fuse` library) combined with periodic health checks:

```elixir
defmodule Catena.HealthChecker do
  use GenServer
  
  @check_interval 5_000
  @failure_threshold 3

  def init(services) do
    # Install fuses for each service
    Enum.each(services, fn {name, _spec} ->
      :fuse.install(name, {{:standard, 2, 10_000}, {:reset, 60_000}})
    end)
    
    schedule_checks()
    {:ok, %{services: services, failures: %{}}}
  end

  def handle_info(:check_health, state) do
    new_state = Enum.reduce(state.services, state, fn {name, spec}, acc ->
      case perform_health_check(spec.health_check) do
        :ok -> 
          handle_service_healthy(name, acc)
        {:error, _reason} ->
          handle_service_unhealthy(name, acc)
      end
    end)
    
    schedule_checks()
    {:noreply, new_state}
  end
  
  defp handle_service_unhealthy(name, state) do
    failures = Map.update(state.failures, name, 1, &(&1 + 1))
    
    if failures[name] >= @failure_threshold do
      :fuse.melt(name)  # Trip circuit breaker
      GenStateMachine.cast(Catena.AvailabilityManager, {:service_down, name})
    end
    
    %{state | failures: failures}
  end
  
  defp handle_service_healthy(name, state) do
    if Map.get(state.failures, name, 0) >= @failure_threshold do
      :fuse.reset(name)  # Reset circuit breaker
      GenStateMachine.cast(Catena.AvailabilityManager, {:service_up, name})
    end
    
    %{state | failures: Map.put(state.failures, name, 0)}
  end
end
```

## Conditional service startup enables degraded boot

The application startup sequence probes dependencies and starts in the highest achievable level:

```elixir
defmodule Catena.Application do
  use Application
  
  def start(_type, _args) do
    # Phase 1: Start critical infrastructure
    {:ok, _} = start_critical_services()
    
    # Phase 2: Probe optional services, start what's available
    available_services = probe_optional_services()
    initial_level = calculate_initial_level(available_services)
    
    Logger.info("Starting Catena at availability level: #{initial_level}")
    
    # Phase 3: Start level-appropriate supervision tree
    children = build_supervision_tree(initial_level, available_services)
    
    opts = [strategy: :one_for_one, name: Catena.Supervisor]
    Supervisor.start_link(children, opts)
  end
  
  defp probe_optional_services do
    optional = [:search, :ml_service, :recommendations]
    
    Enum.reduce(optional, %{}, fn service, acc ->
      status = case probe_service(service) do
        :ok -> :up
        :error -> :down
      end
      Map.put(acc, service, status)
    end)
  end
  
  defp build_supervision_tree(level, services) do
    critical = [
      {Phoenix.PubSub, name: Catena.PubSub},
      {Catena.AvailabilityManager, [level: level, services: services]},
      Catena.Database.Pool,
      Catena.Core.Registry
    ]
    
    level1 = if level in [:l1, :l2] do
      [Catena.Cache.Supervisor, Catena.HealthChecker]
    else
      []
    end
    
    level2 = if level == :l2 do
      for {name, :up} <- services, name in [:search, :ml_service] do
        {service_module(name), []}
      end
    else
      []
    end
    
    critical ++ level1 ++ level2
  end
end
```

## Type-level availability enforcement in Catena

The Catena compiler can enforce availability constraints using graded types. A function annotated with a level requirement cannot call functions requiring higher levels:

```
-- Catena source (hypothetical syntax based on category theory)
-- L0 function: only uses database
query_user : UserId -> Catena[L0] (Maybe User)
query_user uid = 
  Database.fetch("users", uid)

-- L1 function: uses cache, requires L1 availability
cached_query : UserId -> Catena[L1] User
cached_query uid =
  Cache.get_or_fetch(uid, query_user)

-- L2 function: uses ML service, requires L2 availability  
recommend_for : UserId -> Catena[L2] [Item]
recommend_for uid = do
  user <- cached_query uid  -- L1 ⊑ L2, allowed
  prefs <- ML.predict_preferences user
  pure (rank_items prefs)

-- Compile error: L2 requirement used in L1 context
bad_function : UserId -> Catena[L1] [Item]
bad_function uid = recommend_for uid  -- ERROR: L2 > L1
```

The runtime can then use **effect handlers** (algebraic effects as described by Plotkin and Pretnar) to provide level-appropriate implementations:

```elixir
# Runtime effect handler for graceful degradation
defmodule Catena.Runtime.EffectHandler do
  def handle_service_call(service, request, current_level) do
    required_level = service_level(service)
    
    cond do
      level_satisfied?(current_level, required_level) ->
        # Level satisfied, make actual call with circuit breaker
        case :fuse.ask(service, :sync) do
          :ok -> perform_call(service, request)
          :blown -> {:error, :circuit_open}
        end
        
      has_fallback?(service) ->
        # Level not satisfied but fallback available
        {:degraded, get_fallback(service, request)}
        
      true ->
        # No fallback, return error
        {:error, {:unavailable, service, required_level}}
    end
  end
end
```

## Data structures for the complete system

The complete system requires these core data structures working in concert:

```elixir
defmodule Catena.Types do
  # Service registry entry
  defmodule ServiceEntry do
    defstruct [
      :name,
      :pid,
      :level,            # :l0 | :l1 | :l2
      :type,             # :critical | :optional
      :status,           # :starting | :up | :degraded | :down
      :capabilities,     # MapSet of capability atoms
      :dependencies,     # [service_name]
      :circuit_state,    # :closed | :open | :half_open
      :failure_count,
      :last_health_check
    ]
  end
  
  # System availability state
  defmodule SystemState do
    defstruct [
      :level,                    # current availability level
      :services,                 # %{name => ServiceEntry}
      :capability_index,         # %{capability => [service_name]}
      :dependency_graph,         # directed graph of dependencies
      :level_requirements,       # %{level => required_capabilities}
      :transition_history        # [{timestamp, old_level, new_level}]
    ]
  end
  
  # Health check result
  defmodule HealthResult do
    defstruct [
      :service,
      :status,           # :healthy | :degraded | :unhealthy
      :latency_ms,
      :details,          # service-specific health info
      :timestamp
    ]
  end
end
```

## Trade-offs shape implementation choices

Several design decisions involve trade-offs:

**Compile-time versus runtime enforcement**: Full compile-time availability tracking requires sophisticated type system support (graded types, indexed types). For pragmatic implementation, Catena can start with runtime checks and gradually add static verification. The runtime approach is more flexible but misses errors until execution.

**Centralized versus distributed state**: A single AvailabilityManager simplifies reasoning but creates a potential bottleneck. For large deployments, consider **process groups** (`pg`) for distributed state with eventual consistency, accepting that brief windows of inconsistency may exist during transitions.

**Immediate versus debounced transitions**: Immediate level changes can cause flapping if services oscillate. The implementation should include **hysteresis**—requiring sustained failure or recovery before transitioning. The health checker's failure threshold provides this for downgrades; consider a similar success threshold for upgrades.

**Supervision tree structure**: Placing all L2 services under one supervisor means a failing L2 service's restarts count toward the supervisor's max_restarts threshold. Alternative: individual supervisors per service with `temporary` restart strategy, letting the health checker manage recovery rather than supervision restarts.

## Key implementation recommendations

1. **Start with runtime checks, add static typing incrementally**. Implement the AvailabilityManager and health checking first. Add graded types to Catena's compiler as the type system matures.

2. **Use the `fuse` library for circuit breakers**. It's battle-tested in production Erlang systems, ETS-based for performance, and integrates cleanly with OTP patterns.

3. **Broadcast state changes via PubSub**. Phoenix.PubSub provides reliable distribution of availability level changes to all interested processes without tight coupling.

4. **Implement Kubernetes-style probe separation**. Distinguish startup probes (longer timeout, only during initialization), liveness probes (is the process responsive), and readiness probes (should traffic be routed here).

5. **Test degradation paths explicitly**. Use property-based testing to verify that all reachable states preserve system invariants, and that cascading failures stabilize at the correct level.

## References and further reading

The theoretical foundations draw from several key sources. **Katsumata's "Parametric effect monads and semantics of effect systems" (POPL 2014)** provides the formal graded monad definition. **Orchard, Wadler, and Eades' "Unifying graded and parameterised monads" (2020)** extends this to category-graded monads capturing both effect grades and state transitions. The **`effect-monad` package** on Hackage demonstrates practical Haskell implementation.

For BEAM-specific patterns, the **OTP Design Principles** documentation covers supervision strategies exhaustively. The **`fuse` library** provides production-ready circuit breakers. **"Designing for Scalability with Erlang/OTP"** by Cesarini and Vinoski covers fault tolerance patterns in depth.

The microservices patterns draw from **"Release It!" by Michael Nygard** (circuit breakers, bulkheads), the **Resilience4j documentation** (composable resilience patterns), and **Kubernetes probe documentation** (health check semantics). The Netflix technology blog archives contain extensive real-world graceful degradation case studies.

This architecture provides Catena with a principled foundation for graceful degradation—leveraging category theory for formal correctness guarantees while building on BEAM's proven fault tolerance primitives for robust runtime behavior.
